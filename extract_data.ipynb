{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GnTRnD\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to track_data.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://openapi.gg.go.kr/TBZEROCCTVROUTESTM\"\n",
    "api_key = \"a4daa473b93240a3a3d711dfc5cb004d\"\n",
    "\n",
    "# Create an empty list to store all the data dictionaries\n",
    "data = []\n",
    "\n",
    "# Start with the first page\n",
    "page = 1\n",
    "while True:\n",
    "    params = {\n",
    "        \"KEY\": api_key,\n",
    "        \"pIndex\": page,\n",
    "        \"pSize\": 100\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the row elements in the XML response\n",
    "    rows = soup.find_all(\"row\")\n",
    "\n",
    "    if not rows:\n",
    "        # No more data available, break the loop\n",
    "        break\n",
    "\n",
    "    for row in rows:\n",
    "        # Extract the data from each row and create a dictionary\n",
    "        row_data = {}\n",
    "        for element in row.find_all():\n",
    "            row_data[element.name] = element.text.strip()\n",
    "\n",
    "        # Append the dictionary to the list of data\n",
    "        data.append(row_data)\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Extract the column names from the data (assuming all rows have the same columns)\n",
    "column_names = data[0].keys() if data else []\n",
    "\n",
    "# Save the data to a CSV file\n",
    "csv_file = \"track_data.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "    writer.writeheader()  # Write the header row\n",
    "    writer.writerows(data)  # Write the data rows\n",
    "\n",
    "print(f\"Data saved to {csv_file} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
